{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e885b8-d68a-4a2d-95dc-bf3aea03e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59642830-1654-49ba-b733-7ee1a682b2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/.venv/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/media/.venv/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515952b4-a071-4efd-a158-c78205efd088",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_GROUP_SIZE = 128 \n",
    "ZERO_POINT = True \n",
    "W_BIT = 4 \n",
    "VERSION = \"GEMM\" \n",
    "SAFETENSORS = True\n",
    "\n",
    "quant_config = { \"zero_point\": ZERO_POINT, \"q_group_size\": Q_GROUP_SIZE, \"w_bit\": 4, \"version\": VERSION }\n",
    "\n",
    "# Load model\n",
    "model = AutoAWQForCausalLM.from_pretrained(\"google/gemma-2-9b\", torch_dtype=torch.bfloat16, safetensors=SAFETENSORS)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c771a8-4d8b-4854-b05d-67c8ceb1b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize\n",
    "model.quantize(tokenizer, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84986297-72ba-41a0-a6fa-87ff1d056d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_dequantize_tensor(w, scales, zeros = None):\n",
    "        # get repeated count\n",
    "        repeat_count = w.weight.data.shape[-1] // scales.shape[-1]\n",
    "        scales = scales.repeat(1, repeat_count).reshape(w.weight.data.shape)\n",
    "\n",
    "        # dequantize\n",
    "        # if self.zero_point:\n",
    "        zeros = zeros.repeat(1, repeat_count).reshape(w.weight.data.shape)\n",
    "        w = (w.weight.data - zeros) * scales\n",
    "        # else:\n",
    "        # w = w.weight.data * scales\n",
    "\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f86e4b-5b65-4456-9116-b559431c9015",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_path = \"./gemma-2-9b-awq4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb9275dc-53dd-44e3-a2f7-4f1232387e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(quant_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bea2f914-e813-4f30-b81a-5cd0a1e25092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing layers...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 42/42 [00:02<00:00, 16.51it/s]\n",
      "Fusing layers...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 42/42 [00:00<00:00, 192.81it/s]\n"
     ]
    }
   ],
   "source": [
    "model_q = AutoAWQForCausalLM.from_quantized(quant_path, attn_implementation='eager')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
